<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Extracting the most common words in Vietnamese | Trung ƒê·ª©c H√† </title> <meta name="author" content="Trung ƒê·ª©c H√†"> <meta name="description" content="We extract the most common words in Vietnamese by analyzing word frequencies of large documents"> <meta name="keywords" content="blog, projects, programming, nlp, ml"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://devintdha.github.io/blog/2024/extracting-most-common-words-in-vi/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Trung ƒê·ª©c</span> H√† </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Extracting the most common words in Vietnamese</h1> <p class="post-meta"> Created in October 25, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> natural-language-processing</a> ¬† <a href="/blog/tag/vietnamese"> <i class="fa-solid fa-hashtag fa-sm"></i> vietnamese</a> ¬† ¬∑ ¬† <a href="/blog/category/vietnamese-language-learning-tools"> <i class="fa-solid fa-tag fa-sm"></i> vietnamese-language-learning-tools</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post is the first in a series about tools for Vietnamese language learning tools. I will describe, how we can extract the most common words in a number of large documents in the Vietnamese language. This list can then be used to create a vocabulary list for study.</p> <p>For the whole project, see the <a href="/projects/vietnamese-language-tools/">dedicated project page</a>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies_log10.png" sizes="95vw"></source> <img src="/assets/img/projects/VN/word_frequencies_log10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" id="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Word Frequencies on Log10 scale. Sneak peak at what's to come! </div> <blockquote> <p><strong>Summary</strong></p> <p>In this post we will</p> <ul> <li>see where to download and how to process large documents in the Vietnamese language</li> <li>Perform word segmentation to get the correct meanings of the words</li> <li>Count the words to get their frequencies to create a vocabulary list for language learning</li> </ul> <p>I published the code for this post on GitHub <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> </blockquote> <h2 id="goal">Goal</h2> <p>When learning a language, is it useful to know at least <em>some</em> words of the target language to understand it and be able to communicate with other people. But in which order should we learn these new words?</p> <p>As a starting point, I assumed that learning the most frequent words would be pretty good. And indeed, I read some discussions <a class="citation" href="#liPowerLawDistribution2017">[2], [3]</a> where people have a similar approach to learning. In summary, learning the first 1000-2000 most common words lets you understand basic conversations, with more words allowing for more texts. Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a> is painting a pretty bleak picture here, who estimates that you will need about 98% of the most common words or 27,000 to become near fluent!</p> <p>I am aware that just learning the most common words will not suddenly help me become fluent (sadly). But it certainly would help me to read and immerse myself more without having to pick up the dictionary all the time.</p> <p>So I downloaded the <a href="https://Ankiweb.net/shared/info/1903023972" rel="external nofollow noopener" target="_blank">first Anki deck I found on the internet</a> and started learning.</p> <h2 id="problem">Problem</h2> <p>Pretty soon however, I found some problems with the Anki deck:</p> <ol> <li>There is no usage context on the cards, so it is pretty hard to remember them.</li> <li>The quality of translations is lacking for lots of cards</li> <li>Using 5 minutes of research, I have no idea how these words were extracted in the first place. Seems to be quite random at times.</li> </ol> <p>So I set out to improve my learning experience and I wanted to create my own word frequency list for fun with the data I already had.</p> <p><em>Note: While researching for this blog post I realized that I really should have looked deeper into available resources. For example, I could‚Äôve easily uses <a href="https://github.com/rspeer/wordfreq" rel="external nofollow noopener" target="_blank">this available word frequency list</a>. Note sure what happened there and I missed it. Whoops ü§¶! But at least I learned lots of things along the way.</em></p> <h3 id="where-to-find-which-words">Where To Find Which Words</h3> <p>So first, we will need to find some Vietnamese texts. I will describe which ones in the <a href="#corpora">Corpora</a> section. For people unfamiliar with this term: According to Wiktionary a <em>corpus</em> in our context is ‚Äúa collection of writings in form of an electronic database used for linguistic analyses‚Äù.</p> <p>Moreover, we can‚Äôt just use the raw words from the text. While the Vietnamese language does not have inflections like in English, we have to consider compound words, which Vietnamese utilizes heavily.</p> <p>To illustrate this, let‚Äôs take the following example sentence:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T·ªëi s·∫Ω th√†nh c√¥ng. (I will succeed.)
</code></pre></div></div> <p>Here, we want to consider the words <code class="language-plaintext highlighter-rouge">T·ªëi</code>, <code class="language-plaintext highlighter-rouge">s·∫Ω</code> and <code class="language-plaintext highlighter-rouge">th√†nh c√¥ng</code> as it means succeed, instead of its compounds. This task is called <em>text or word segmentation</em> and I will describe a way to do this quickly in the <a href="#word-segmentation">Word Segmentation</a> section.</p> <p>After this, we need to analyze the frequency of each segmented word and need to decide for a cut off point. Although probably not really scientific, I choose the previously mentioned 98% mark for the rest of this post as benchmark for the word extraction.</p> <h2 id="solution">Solution</h2> <p>So to go ahead and extract these words we will need to:</p> <ol> <li>Select, download and pre-process the corpora.</li> <li>Apply word segmentation to all sentences to find the correct words.</li> <li>Count, merge and analyze the word frequencies for all documents</li> </ol> <h3 id="1-corpora">1. Corpora</h3> <p>First, I want to mention that there is a great GitHub repository which summarizes various language processing resources for Vietnamese called ‚Äúawesome Vietnamese NLP‚Äù <a class="citation" href="#huynhVndeeAwsomevietnamesenlp2024">[4]</a> I wanted to include two types of corpus: First, texts that are more formal such as the news and Wikipedia and second more casual conversations. For this I decided on the following corpora:</p> <ol> <li>Binhvq News Corpus <a class="citation" href="#vuongquocBinhvqNewscorpusCorpus2024">[5]</a>, which includes news <ul> <li>About 20¬†GB with about 3 billion words</li> </ul> </li> <li>viwik18 <a class="citation" href="#NTT123Viwik18Vietnamese2018">[6]</a>, which is a dump of the vietnamese wikipedia from 2018 <ul> <li>About 98¬†MB with about 27 million words</li> </ul> </li> <li>Facebook comment corpus which was also included in the repo of the Binhvq News Corpus. <ul> <li>About 444 MB with about 81 million words</li> </ul> </li> <li>opensubtitles.org.Actually.Open.Edition <a class="citation" href="#5719123SubtitlesOpensubtitlesorg">[7]</a> which is a (questionable?) dump of subtitle files from Open Subtitles <ul> <li>About 379 MB with about 122 million words</li> </ul> </li> </ol> <p>We can just download the corpora and start working with them, as they already are in a readable format. Basically they are just raw text, split across many small files. Except the subtitle files, which need some extra processing described in the next section.</p> <p>I estimated the size with the <code class="language-plaintext highlighter-rouge">du -sh</code> command and used <code class="language-plaintext highlighter-rouge">wc -w</code> to count the number of words. So in total we have about 21 GB worth of data with more than 3 billion words.</p> <h4 id="processing-open-subtitles-files">Processing Open Subtitles Files</h4> <p>Processing the subtitle files involves a bit more work. I will only summarize the processing of this corpus, but I will probably write a whole blog post about it at some later point in time. To process this dataset, I applied the following steps:</p> <ol> <li>Extract all subtitles zips which have the Vietnamese language code (vi)</li> <li>Parse the SRT files and join them together to a single line. This is because a sentence might be distributed over multiple lines.</li> <li>Use a <em>sentence segmentation model</em> to extract the sentences and write them to a text file. <ul> <li>A sentence segmentation model separates a single line of text into sentences.</li> </ul> </li> </ol> <p>With the text available, we proceed with the next step of processing: the word segmentation described earlier.</p> <h3 id="2-word-segmentation">2. Word Segmentation</h3> <p>To perform the word segmentation, I chose to go for a non-deep learning approach for which I have two reasons. First, I thought using a deep learning model would take forever to process everything. Second, after a quick search I found a paper <a class="citation" href="#vuVnCoreNLPVietnameseNatural2018">[8]</a> and its GitHub Repo <a class="citation" href="#corenlpVncorenlpVnCoreNLP2024">[9]</a> written in Java which describes a ‚Äútransformation rule-based learning model‚Äù. I‚Äôm not going to pretend to know what this exactly means, but ‚Äúrule-based‚Äù sounds fast, and it‚Äôs supposed to be accurate enough.</p> <p>I was not overly concerned with accuracy and hoped due to the size of the texts that it would average out. I was mostly concerned with getting it done, so I can actually stop procrastinating and start learning Vietnamese.</p> <p>Let‚Äôs take a look at an example how the results look like. If we have the following input (The police does not have time to continue verifying.)</p> <p><code class="language-plaintext highlighter-rouge">C√¥ng an ph∆∞·ªùng kh√¥ng c√≥ th·ªùi gian x√°c minh ti·∫øp.</code></p> <p>then the result will become</p> <p><code class="language-plaintext highlighter-rouge">C√¥ng_an ph∆∞·ªùng kh√¥ng c√≥ th·ªùi_gian x√°c_minh ti·∫øp .</code></p> <p>The compound words are combined with an underscore, which allows us to easily continue processing it.</p> <h4 id="implementation-details">Implementation Details</h4> <p>This section goes in a bit deeper how I accomplished this. Feel free to skip it.</p> <p>I haven‚Äôt worked with concurrent Java code before and I feel a bit clunky with it. Once I started to use <a href="https://www.scala-lang.org/" rel="external nofollow noopener" target="_blank">Scala</a> for work, I could never look back. But I never really worked with concurrency in Scala before either at this point and was a bit intimidated by it.</p> <p>Turns out, that it is actually quite simple in this case! All you have to do is use <a href="https://docs.scala-lang.org/overviews/parallel-collections/overview.html" rel="external nofollow noopener" target="_blank">parallel collections</a>. If we apply the algorithm in parallel to the files inside these collections, we can quite easily achieve some speedup (also known as an <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" rel="external nofollow noopener" target="_blank">embarrassingly parallel</a> problem). The resulting code is quite small, and I published it <a href="https://github.com/DevinTDHa/vn-nlp-exp/blob/main/rdrsegmenter_wfreqs/VnCoreNLPScala/src/main/scala/ProcessFolder.scala" rel="external nofollow noopener" target="_blank">in the repo for this post</a>.</p> <p>Using this parallelized code, I was able to completely process all corpora quite quickly. I can‚Äôt be bothered to run it again, but it must have been less than 2 hours to process everything.</p> <h3 id="3-analyzing-word-frequencies">3. Analyzing Word Frequencies</h3> <p>So previously, we saw that Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a> estimates we need 98% of the most common words to reach near near-native level, which I chose as a benchmark. Additionally, I want to group the words by the first, second, etc. thousand most common, so I can better track my progress.</p> <p>To accomplish this, the rest of the processing is actually quite easy. What we need to do at this point is</p> <ol> <li>Count all segmented words from all documents</li> <li>Merge them to a single file (and filter them by a list of valid words from a dictionary)</li> <li>Split this large file and group by the n-th thousand most common words</li> </ol> <p>If we just use the raw words, we will have some non-word characters and other nonsense in the list. To filter it I used a word list constructed from the following dictionaries:</p> <ol> <li>Wiktextract <a class="citation" href="#ylonenWiktextractWiktionaryMachineReadable2022">[10]</a> and its GitHub repo <a class="citation" href="#ylonenTatuylonenWiktextract2024">[11]</a> <ul> <li>This is a great project, that parses Wiktionary XML dumps regularly and publishes them in a machine-readable JSONL format. It also includes entries for all languages. I will also use this for other parts of the project.</li> </ul> </li> <li>The Free Vietnamese Dictionary Project <a class="citation" href="#hongocFreeVietnameseDictionary2004">[12]</a> <ul> <li>This seems to be a rather old project for a free Vietnamese dictionary from Uni Leipzig. The data available for download, but it‚Äôs a bit of a hassle to use it directly. I wrote a parser for it to convert it to the same JSONL format as the Wiktextract project <a class="citation" href="#haDevinTDHaExporterFreeVietnameseDictionaryProject2024">[13]</a>.</li> </ul> </li> </ol> <p>All the steps above can be quite easily achieved by a couple of <a href="https://github.com/DevinTDHa/vn-nlp-exp/tree/main/rdrsegmenter_wfreqs/python" rel="external nofollow noopener" target="_blank">python</a> and <a href="https://github.com/DevinTDHa/vn-nlp-exp/tree/main/rdrsegmenter_wfreqs/shell" rel="external nofollow noopener" target="_blank">shell</a> scripts that can be found my repo <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> <p>After all of that, we will have a folder which contains the most frequent words split conveniently into files of 1000 lines each. This folder will look like this:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls
</span>most_frequent_01.txt  most_frequent_06.txt  most_frequent_11.txt  most_frequent_16.txt  most_frequent_21.txt
most_frequent_02.txt  most_frequent_07.txt  most_frequent_12.txt  most_frequent_17.txt  most_frequent_22.txt
most_frequent_03.txt  most_frequent_08.txt  most_frequent_13.txt  most_frequent_18.txt  most_frequent_23.txt
most_frequent_04.txt  most_frequent_09.txt  most_frequent_14.txt  most_frequent_19.txt  most_frequent_24.txt
most_frequent_05.txt  most_frequent_10.txt  most_frequent_15.txt  most_frequent_20.txt
</code></pre></div></div> <p>And let‚Äôs see how the first 20 entries of the word frequencies look like :</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v√†,41599096
c·ªßa,41009489
c√°c,29460924
l√†,29311617
c√≥,27590205
trong,27345997
ƒë∆∞·ª£c,25208913
ƒë√£,24565107
cho,23718032
v·ªõi,22242832
kh√¥ng,20117105
m·ªôt,19513294
ng∆∞·ªùi,18231853
nh·ªØng,18118913
ƒë·ªÉ,14998163
khi,14225072
n√†y,14098654
·ªü,13816430
v·ªÅ,13204218
ƒë·∫øn,13017067
</code></pre></div></div> <p><em>Note: (in <code class="language-plaintext highlighter-rouge">most_frequent_01.txt</code> the counts will be removed, for a different feature. See <a href="#conclusion-and-future-work">Future Work</a>.)</em></p> <p>That‚Äôs what we want. Neat! The frequency lists of each corpus, as well as the merged one can be found in the releases of my GitHub repo <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> <h4 id="frequency-distribution">Frequency Distribution</h4> <p>Having all extracted this data, I also wanted to see how the distribution of the words looked like. Additionally, I wanted to calculate amount of words needed to hit the thresholds (80%, 90%, 95%, 98%, 99%, 99.5%) mentioned in the blog post by Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a>. For this, I wrote a simple <a href="https://github.com/DevinTDHa/vn-nlp-exp/blob/main/rdrsegmenter_wfreqs/python/generate_freq_plot.py" rel="external nofollow noopener" target="_blank">python script</a> which plots the rank of the word against its frequency and marks the thresholds. This is the result:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies.png" sizes="95vw"></source> <img src="/assets/img/projects/VN/word_frequencies.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Plot Of Word Frequencies" id="Plot Of Word Frequencies" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure: Distribution of word frequencies</figcaption> </figure> </div> </div> <p>We see that we can‚Äôt see much because the distribution is very dense at the most common words. However, it seems like the vocabulary size for covering the threshold seems to be lower in Vietnamese. We previously said saw that we need 27,000 to hit near-native level. If we assume the same threshold percentage in Vietnamese, then we would only need 7,000.</p> <p>As a bonus I plotted the whole thing again on a Log10 scale, to get a prettier plot:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies_log10.png" sizes="95vw"></source> <img src="/assets/img/projects/VN/word_frequencies_log10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Plot of Word Frequencies on Log10 scale" id="Plot of Word Frequencies on Log10 scale" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure: Word Frequencies on a Log10 scale. I think it looks beautiful.</figcaption> </figure> </div> </div> <p>How we can read this is we look at the tenth power at the index to get a feel for the magnitude. For example, at index 0 we have more than \(10^7\), so tens of millions of occurences, while at the 98% mark we are at about \(10^4\) meaning tens of thousands.</p> <h2 id="conclusion-and-future-work">Conclusion and Future Work</h2> <p>To conclude this post, we have taken a look at how to process corpora in the Vietnamese language and extract its word frequencies using tools found on GitHub, as well as some scripts that I wrote and published on GitHub <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>. After that, we found out how many of the most frequent words we need to reach a certain level of proficiency, which are about 7,000.</p> <p>In the next post we will tackle next problem. Now that we have all the words it would be a <em>real</em> pain to add them manually add them to Anki. So what can we do? Of course spend a significant amount of time to automate it. It‚Äôs actually pretty worth it though this time, I swear!</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <p><span id="DevinTDHaVnnlpexpNLP">[1]‚ÄúDevinTDHa/vn-Nlp-Exp: NLP Experiments for the Vietnamese Language.‚Äù Accessed: Oct. 30, 2024. [Online]. Available at: <a href="https://github.com/DevinTDHa/vn-nlp-exp/tree/main" rel="external nofollow noopener" target="_blank">https://github.com/DevinTDHa/vn-nlp-exp/tree/main</a>.</span></p> </li> <li> <p><span id="liPowerLawDistribution2017">[2]B. Li, ‚ÄúThe Power Law Distribution and the Harsh Reality of Language Learning.‚Äù Lucky‚Äôs Notes, Jul. 12, 2017, Accessed: Oct. 27, 2024. [Online]. Available at: <a href="https://luckytoilet.wordpress.com/2017/07/12/the-power-law-distribution-and-the-harsh-reality-of-language-learning/" rel="external nofollow noopener" target="_blank">https://luckytoilet.wordpress.com/2017/07/12/the-power-law-distribution-and-the-harsh-reality-of-language-learning/</a>.</span></p> </li> <li> <p><span id="SizeVocabularySet2021">[3]‚ÄúThe Size of Vocabulary to Set as a Goal. - A Language Learners‚Äô Forum.‚Äù 2021, Accessed: Oct. 28, 2024. [Online]. Available at: <a href="https://forum.language-learners.org/viewtopic.php?t=16463" rel="external nofollow noopener" target="_blank">https://forum.language-learners.org/viewtopic.php?t=16463</a>.</span></p> </li> <li> <p><span id="huynhVndeeAwsomevietnamesenlp2024">[4]D. Huynh, ‚ÄúVndee/Awsome-Vietnamese-Nlp.‚Äù Oct. 21, 2024, Accessed: Oct. 27, 2024. [Online]. Available at: <a href="https://github.com/vndee/awsome-vietnamese-nlp" rel="external nofollow noopener" target="_blank">https://github.com/vndee/awsome-vietnamese-nlp</a>.</span></p> </li> <li> <p><span id="vuongquocBinhvqNewscorpusCorpus2024">[5]B. V∆∞∆°ng Qu·ªëc, ‚ÄúBinhvq/News-Corpus: Corpus Ti·∫øng Vi·ªát.‚Äù 2024, Accessed: Oct. 27, 2024. [Online]. Available at: <a href="https://github.com/binhvq/news-corpus" rel="external nofollow noopener" target="_blank">https://github.com/binhvq/news-corpus</a>.</span></p> </li> <li> <p><span id="NTT123Viwik18Vietnamese2018">[6]‚ÄúNTT123/Viwik18: Vietnamese Text Dataset - Wikipedia vi 2018.‚Äù 2018, Accessed: Oct. 27, 2024. [Online]. Available at: <a href="https://github.com/NTT123/viwik18" rel="external nofollow noopener" target="_blank">https://github.com/NTT123/viwik18</a>.</span></p> </li> <li> <p><span id="5719123SubtitlesOpensubtitlesorg">[7]‚Äú‚Äò5,719,123 Subtitles from Opensubtitles.Org‚Äô or ‚ÄòOpensubtitles.Org.Actually.Open.Edition.2022.07.25.‚Äô‚Äù Accessed: Oct. 29, 2024. [Online]. Available at: <a href="http://archive.org/details/opensubtitles.org.Actually.Open.Edition.2022.07.25" rel="external nofollow noopener" target="_blank">http://archive.org/details/opensubtitles.org.Actually.Open.Edition.2022.07.25</a>.</span></p> </li> <li> <p><span id="vuVnCoreNLPVietnameseNatural2018">[8]T. Vu, D. Q. Nguyen, M. Dras, and M. Johnson, ‚ÄúVnCoreNLP: A Vietnamese Natural Language Processing Toolkit,‚Äù in <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</i>, Jun. 2018, pp. 56‚Äì60, doi: 10.18653/v1/N18-5012.</span></p> </li> <li> <p><span id="corenlpVncorenlpVnCoreNLP2024">[9]V. CoreNLP, ‚ÄúVncorenlp/VnCoreNLP.‚Äù Oct. 24, 2024, Accessed: Oct. 29, 2024. [Online]. Available at: <a href="https://github.com/vncorenlp/VnCoreNLP" rel="external nofollow noopener" target="_blank">https://github.com/vncorenlp/VnCoreNLP</a>.</span></p> </li> <li> <p><span id="ylonenWiktextractWiktionaryMachineReadable2022">[10]T. Ylonen, ‚ÄúWiktextract: Wiktionary as Machine-Readable Structured Data,‚Äù in <i>Proceedings of the Thirteenth Language Resources and Evaluation Conference</i>, Jun. 2022, pp. 1317‚Äì1325, Accessed: Oct. 30, 2024. [Online]. Available at: <a href="https://aclanthology.org/2022.lrec-1.140" rel="external nofollow noopener" target="_blank">https://aclanthology.org/2022.lrec-1.140</a>.</span></p> </li> <li> <p><span id="ylonenTatuylonenWiktextract2024">[11]T. Ylonen, ‚ÄúTatuylonen/Wiktextract.‚Äù Oct. 30, 2024, Accessed: Oct. 30, 2024. [Online]. Available at: <a href="https://github.com/tatuylonen/wiktextract" rel="external nofollow noopener" target="_blank">https://github.com/tatuylonen/wiktextract</a>.</span></p> </li> <li> <p><span id="hongocFreeVietnameseDictionary2004">[12]D. Ho Ngoc, ‚ÄúFree Vietnamese Dictionary Project.‚Äù 2004, Accessed: Oct. 30, 2024. [Online]. Available at: <a href="https://www.informatik.uni-leipzig.de/" rel="external nofollow noopener" target="_blank">https://www.informatik.uni-leipzig.de/</a>¬†duc/Dict/install.html.</span></p> </li> <li> <p><span id="haDevinTDHaExporterFreeVietnameseDictionaryProject2024">[13]D. Ha, ‚ÄúDevinTDHa/Exporter-For-The-Free-Vietnamese-Dictionary-Project.‚Äù Oct. 12, 2024, Accessed: Oct. 30, 2024. [Online]. Available at: <a href="https://github.com/DevinTDHa/Exporter-For-The-Free-Vietnamese-Dictionary-Project" rel="external nofollow noopener" target="_blank">https://github.com/DevinTDHa/Exporter-For-The-Free-Vietnamese-Dictionary-Project</a>.</span></p> </li> </ol> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2024 Trung ƒê·ª©c H√†. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A selection of projects I did in my free time.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"Check out my repos on GitHub. I usually publish the source code for my projects there.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"This page is under construction. But you can still download my resume using the button on the right.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-extracting-the-most-common-words-in-vietnamese",title:"Extracting the most common words in Vietnamese",description:"We extract the most common words in Vietnamese by analyzing word frequencies of large documents",section:"Posts",handler:()=>{window.location.href="/blog/2024/extracting-most-common-words-in-vi/"}},{id:"post-hello-world",title:"Hello World",description:"The first post on this blog!",section:"Posts",handler:()=>{window.location.href="/blog/2024/hello-world/"}},{id:"news-finally-found-the-time-to-create-a-blog-i-had-this-on-my-to-do-list-for-a-while-but-now-i-have-the-right-occasion-to-do-so-in-the-next-days-i-will-create-some-posts-describing-how-i-created-my-vietnamese-anki-decks",title:"Finally found the time to create a blog! I had this on my...",description:"",section:"News"},{id:"projects-tools-for-vietnamese-language-learning",title:"Tools for Vietnamese Language Learning",description:"Series of posts describing tools for Vietnamese language learning",section:"Projects",handler:()=>{window.location.href="/projects/vietnamese-language-tools/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>