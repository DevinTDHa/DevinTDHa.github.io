<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://devintdha.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://devintdha.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-17T21:16:30+00:00</updated><id>https://devintdha.github.io/feed.xml</id><title type="html">blank</title><subtitle>Trung ƒê·ª©c H√† - Blog and other writings </subtitle><entry><title type="html">Extracting the most common words in Vietnamese</title><link href="https://devintdha.github.io/blog/2024/extracting-most-common-words-in-vi/" rel="alternate" type="text/html" title="Extracting the most common words in Vietnamese"/><published>2024-10-25T00:00:00+00:00</published><updated>2024-10-25T00:00:00+00:00</updated><id>https://devintdha.github.io/blog/2024/extracting-most-common-words-in-vi</id><content type="html" xml:base="https://devintdha.github.io/blog/2024/extracting-most-common-words-in-vi/"><![CDATA[<p>This post is the first in a series about tools for Vietnamese language learning tools. I will describe how we can extract the most common words for a number of large documents in the Vietnamese language. This list can then be used to create a vocabulary list for study.</p> <p>For the whole project, see the <a href="/projects/vietnamese-language-tools/">dedicated project page</a>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies_log10.png" sizes="95vw"/> <img src="/assets/img/projects/VN/word_frequencies_log10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" id="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Word Frequencies on Log10 scale. Sneak peak at what's to come! </div> <blockquote> <p><strong>Summary</strong></p> <p>In this post we will</p> <ul> <li>See where to download and how to process large documents in the Vietnamese language.</li> <li>Perform word segmentation to get the correct meanings of the words</li> <li>Count the words to get their frequencies to create a vocabulary list for language learning.</li> </ul> <p>I published the code for this post on GitHub <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> </blockquote> <h2 id="goal">Goal</h2> <p>When learning a language, it is useful to know at least <em>some</em> words of the target language to understand it and be able to communicate with other people. But in which order should we learn these new words?</p> <p>As a starting point, I assumed that learning the most frequent words would be pretty good. And indeed, I read some discussions <a class="citation" href="#liPowerLawDistribution2017">[2], [3]</a> where people have a similar approach to learning. In summary, learning the first 1000-2000 most common words lets you understand basic conversations, with more words allowing for more texts. Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a> is painting a pretty bleak picture here, who estimates that you will need about 98% of the most common words, or 27,000 to become near-native!</p> <p>I am aware that just learning the most common words will not suddenly help me become fluent (sadly). But it certainly would help me to read and immerse myself more without having to pick up the dictionary all the time.</p> <p>So I downloaded the <a href="https://Ankiweb.net/shared/info/1903023972">first Anki deck I found on the internet</a> and started learning.</p> <h2 id="problem">Problem</h2> <p>Pretty soon however, I found some problems with the Anki deck:</p> <ol> <li>There is no usage context on the cards, so it is pretty hard to remember them.</li> <li>The quality of translations is lacking for lots of cards.</li> <li>Using 5 minutes of research, I have no idea how these words were extracted in the first place. Seems to be quite random at times.</li> </ol> <p>So I set out to improve my learning experience, and I wanted to create my own word frequency list for fun with the data I already had.</p> <p><em>Note: While researching for this blog post, I realized that I really should have looked deeper into available resources. For example, I could‚Äôve easily used <a href="https://github.com/rspeer/wordfreq">this available word frequency list</a>. Note sure what happened there and I missed it. Whoops ü§¶! But at least I learned lots of things along the way.</em></p> <h3 id="where-to-find-which-words">Where to Find Which Words</h3> <p>So first, we will need to find some Vietnamese texts. I will describe which ones in the <a href="#corpora">Corpora</a> section. For people unfamiliar with this term: According to Wiktionary, a <em>corpus</em> in our context is ‚Äúa collection of writings in the form of an electronic database used for linguistic analyses‚Äù.</p> <p>Moreover, we can‚Äôt just use the raw words from the text. While the Vietnamese language does not have inflections like in English, we have to consider compound words, which Vietnamese utilizes heavily.</p> <p>To illustrate this, let‚Äôs take the following example sentence:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T√¥i s·∫Ω th√†nh c√¥ng. (I will succeed.)
</code></pre></div></div> <p>Here, we want to consider the words <code class="language-plaintext highlighter-rouge">T√¥i</code>, <code class="language-plaintext highlighter-rouge">s·∫Ω</code> and <code class="language-plaintext highlighter-rouge">th√†nh c√¥ng</code> as it means succeed, instead of its compounds. This task is called <em>text or word segmentation</em> and I will describe a way to do this quickly in the <a href="#word-segmentation">Word Segmentation</a> section.</p> <p>After this, we need to analyze the frequency of each segmented word and need to decide on a cut-off point. Although probably not really scientific, I chose the previously mentioned 98% mark for the rest of this post as a benchmark for the word extraction.</p> <h2 id="solution">Solution</h2> <p>So to go ahead and extract these words we will need to:</p> <ol> <li>Select, download and preprocess the corpora.</li> <li>Apply word segmentation to all sentences to find the correct words.</li> <li>Count, merge and analyze the word frequencies for all documents.</li> </ol> <h3 id="1-corpora">1. Corpora</h3> <p>First, I want to mention that there is a great GitHub repository that summarizes various language processing resources for Vietnamese called ‚Äúawesome Vietnamese NLP‚Äù <a class="citation" href="#huynhVndeeAwsomevietnamesenlp2024">[4]</a> I wanted to include two types of corpus: First, texts that are more formal such as the news and Wikipedia, and second more casual conversations. For this I decided on the following corpora:</p> <ol> <li>Binhvq News Corpus <a class="citation" href="#vuongquocBinhvqNewscorpusCorpus2024">[5]</a>, which includes news <ul> <li>About 20 GB with about 3 billion words</li> </ul> </li> <li>viwik18 <a class="citation" href="#NTT123Viwik18Vietnamese2018">[6]</a>, which is a dump of the Vietnamese Wikipedia from 2018 <ul> <li>About 98 MB with about 27 million words</li> </ul> </li> <li>Facebook comment corpus, which was also included in the repo of the Binhvq News Corpus. <ul> <li>About 444 MB with about 81 million words</li> </ul> </li> <li>opensubtitles.org.Actually.Open.Edition <a class="citation" href="#5719123SubtitlesOpensubtitlesorg">[7]</a> which is a (questionable?) dump of subtitle files from Open Subtitles <ul> <li>About 379 MB with about 122 million words</li> </ul> </li> </ol> <p>We can just download the corpora and start working with them, as they already are in a readable format. Basically, they are just raw text, split across many small files. Except the subtitle files, which need some extra processing described in the next section.</p> <p>I estimated the size with the <code class="language-plaintext highlighter-rouge">du -sh</code> command and used <code class="language-plaintext highlighter-rouge">wc -w</code> to count the number of words. So in total we have about 21 GB worth of data with more than 3 billion words.</p> <h4 id="processing-open-subtitles-files">Processing Open Subtitles Files</h4> <p>Processing the subtitle files involves a bit more work. I will only summarize the processing of this corpus, but I will probably write a whole blog post about it at some later point in time. To process this dataset, I applied the following steps:</p> <ol> <li>Extract all subtitles zips that have the Vietnamese language code (vi)</li> <li>Parse the SRT files and join them together to a single line. This is because a sentence might be distributed over multiple lines.</li> <li>Use a <em>sentence segmentation model</em> to extract the sentences and write them to a text file. <ul> <li>A sentence segmentation model separates a single line of text into sentences.</li> </ul> </li> </ol> <p>With the text available, we proceed with the next step of processing: the word segmentation described earlier.</p> <h3 id="2-word-segmentation">2. Word Segmentation</h3> <p>To perform the word segmentation, I chose to go for a non-deep learning approach, for which I have two reasons. First, I thought using a deep learning model would take forever to process everything. Second, after a quick search I found a paper <a class="citation" href="#vuVnCoreNLPVietnameseNatural2018">[8]</a> and its GitHub repo <a class="citation" href="#corenlpVncorenlpVnCoreNLP2024">[9]</a> written in Java that describes a ‚Äútransformation rule-based learning model‚Äù. I‚Äôm not going to pretend to know what this exactly means, but ‚Äúrule-based‚Äù sounds fast, and it‚Äôs supposed to be accurate enough.</p> <p>I was not overly concerned with accuracy and hoped, due to the size of the texts, that it would average out. I was mostly concerned with getting it done, so I can actually stop procrastinating and start learning Vietnamese.</p> <p>Let‚Äôs take a look at an example of what the results look like. If we have the following input (The police do not have time to continue verifying.)</p> <p><code class="language-plaintext highlighter-rouge">C√¥ng an ph∆∞·ªùng kh√¥ng c√≥ th·ªùi gian x√°c minh ti·∫øp.</code></p> <p>then the result will become</p> <p><code class="language-plaintext highlighter-rouge">C√¥ng_an ph∆∞·ªùng kh√¥ng c√≥ th·ªùi_gian x√°c_minh ti·∫øp .</code></p> <p>The compound words are combined with an underscore, which allows us to easily continue processing it.</p> <h4 id="implementation-details">Implementation Details</h4> <p>This section goes in a bit deeper on how I accomplished this. Feel free to skip it.</p> <p>I haven‚Äôt worked with concurrent Java code before, and I feel a bit clunky with it. Once I started to use <a href="https://www.scala-lang.org/">Scala</a> for work, I could never look back. But I never really worked with concurrency in Scala before either at this point and was a bit intimidated by it.</p> <p>It turns out, that it is actually quite simple in this case! All you have to do is use <a href="https://docs.scala-lang.org/overviews/parallel-collections/overview.html">parallel collections</a>. If we apply the algorithm in parallel to the files inside these collections, we can quite easily achieve some speedup (also known as an <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a> problem). The resulting code is quite small, and I published it <a href="https://github.com/DevinTDHa/vn-nlp-exp/blob/main/rdrsegmenter_wfreqs/VnCoreNLPScala/src/main/scala/ProcessFolder.scala">in the repo for this post</a>.</p> <p>Using this parallelized code, I was able to completely process all corpora quite quickly. I can‚Äôt be bothered to run it again, but it must have been less than 2 hours to process everything.</p> <h3 id="3-analyzing-word-frequencies">3. Analyzing Word Frequencies</h3> <p>So previously, we saw that Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a> estimates we need 98% of the most common words to reach near near-native level, which I chose as a benchmark. Additionally, I want to group the words by the first, second, etc. thousand most common, so I can better track my progress.</p> <p>To accomplish this, the rest of the processing is actually quite easy. What we need to do at this point is</p> <ol> <li>Count all segmented words from all documents.</li> <li>Merge them to a single file (and filter them by a list of valid words from a dictionary).</li> <li>Split this large file and group by the n-th thousand most common words.</li> </ol> <p>If we just use the raw words, we will have some non-word characters and other nonsense in the list. To filter it, I used a word list constructed from the following dictionaries:</p> <ol> <li>Wiktextract <a class="citation" href="#ylonenWiktextractWiktionaryMachineReadable2022">[10]</a> and its GitHub repo <a class="citation" href="#ylonenTatuylonenWiktextract2024">[11]</a> <ul> <li>This is a great project that parses Wiktionary XML dumps regularly and publishes them in a machine-readable JSONL format. It also includes entries for all languages. I will also use this for other parts of the project.</li> </ul> </li> <li>The Free Vietnamese Dictionary Project <a class="citation" href="#hongocFreeVietnameseDictionary2004">[12]</a> <ul> <li>This seems to be a rather old project for a free Vietnamese dictionary from Uni Leipzig. The data is available for download, but it‚Äôs a bit of a hassle to use it directly. I wrote a parser for it to convert it to the same JSONL format as the Wiktextract project <a class="citation" href="#haDevinTDHaExporterFreeVietnameseDictionaryProject2024">[13]</a>.</li> </ul> </li> </ol> <p>All the steps above can be quite easily achieved by a couple of <a href="https://github.com/DevinTDHa/vn-nlp-exp/tree/main/rdrsegmenter_wfreqs/python">Python</a> and <a href="https://github.com/DevinTDHa/vn-nlp-exp/tree/main/rdrsegmenter_wfreqs/shell">Shell</a> scripts that can be found in my repo <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> <p>After all of that, we will have a folder that contains the most frequent words, split conveniently into files of 1000 lines each. This folder will look like this:</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">ls
</span>most_frequent_01.txt  most_frequent_06.txt  most_frequent_11.txt  most_frequent_16.txt  most_frequent_21.txt
most_frequent_02.txt  most_frequent_07.txt  most_frequent_12.txt  most_frequent_17.txt  most_frequent_22.txt
most_frequent_03.txt  most_frequent_08.txt  most_frequent_13.txt  most_frequent_18.txt  most_frequent_23.txt
most_frequent_04.txt  most_frequent_09.txt  most_frequent_14.txt  most_frequent_19.txt  most_frequent_24.txt
most_frequent_05.txt  most_frequent_10.txt  most_frequent_15.txt  most_frequent_20.txt
</code></pre></div></div> <p>And let‚Äôs see how the first 20 entries of the word frequencies look like :</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v√†,41599096
c·ªßa,41009489
c√°c,29460924
l√†,29311617
c√≥,27590205
trong,27345997
ƒë∆∞·ª£c,25208913
ƒë√£,24565107
cho,23718032
v·ªõi,22242832
kh√¥ng,20117105
m·ªôt,19513294
ng∆∞·ªùi,18231853
nh·ªØng,18118913
ƒë·ªÉ,14998163
khi,14225072
n√†y,14098654
·ªü,13816430
v·ªÅ,13204218
ƒë·∫øn,13017067
</code></pre></div></div> <p><em>Note: In <code class="language-plaintext highlighter-rouge">most_frequent_01.txt</code> the counts will be removed for a different feature. See <a href="#conclusion-and-future-work">Future Work</a>.</em></p> <p>That‚Äôs what we want. Neat! The frequency lists of each corpus, as well as the merged one, can be found in the releases of my GitHub repo <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>.</p> <h4 id="frequency-distribution">Frequency Distribution</h4> <p>Having all extracted this data, I also wanted to see what the distribution of the words looked like. Additionally, I wanted to calculate the amount of words needed to hit the thresholds (80%, 90%, 95%, 98%, 99%, 99.5%) mentioned in the blog post by Li <a class="citation" href="#liPowerLawDistribution2017">[2]</a>. For this, I wrote a simple <a href="https://github.com/DevinTDHa/vn-nlp-exp/blob/main/rdrsegmenter_wfreqs/python/generate_freq_plot.py">Python script</a> which plots the rank of the word against its frequency and marks the thresholds. This is the result:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies.png" sizes="95vw"/> <img src="/assets/img/projects/VN/word_frequencies.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Plot Of Word Frequencies" id="Plot Of Word Frequencies" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure: Distribution of word frequencies</figcaption> </figure> </div> </div> <p>We see that we can‚Äôt see much because the distribution is very dense at the most common words. However, it seems like the vocabulary size for covering the threshold seems to be lower in Vietnamese. We previously said we need 27,000 to hit near-native level. If we assume the same threshold percentage in Vietnamese, then we would only need 7,000.</p> <p>As a bonus, I plotted the whole thing again on a Log10 scale to get a prettier plot:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/VN/word_frequencies_log10.png" sizes="95vw"/> <img src="/assets/img/projects/VN/word_frequencies_log10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Plot of Word Frequencies on Log10 scale" id="Plot of Word Frequencies on Log10 scale" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure: Word Frequencies on a Log10 scale. I think it looks beautiful.</figcaption> </figure> </div> </div> <p>How we can read this is we look at the tenth power at the index to get a feel for the magnitude. For example, at index 0, we have more than \(10^7\), so tens of millions of occurences, while at the 98% mark we are at about \(10^4\) meaning tens of thousands.</p> <h2 id="conclusion-and-future-work">Conclusion and Future Work</h2> <p>To conclude this post, we have taken a look at how to process corpora in the Vietnamese language and extract its word frequencies using tools found on GitHub, as well as some scripts that I wrote and published on GitHub <a class="citation" href="#DevinTDHaVnnlpexpNLP">[1]</a>. After that, we found out how many of the most frequent words we need to reach a certain level of proficiency, which are about 7,000.</p> <p>In the next post, we will tackle the next problem. Now that we have all the words, it would be a <em>real</em> pain to manually add them to Anki. So what can we do? Of course, spend a significant amount of time automating it. It‚Äôs actually pretty worth it though, I swear!</p>]]></content><author><name></name></author><category term="vietnamese-language-learning-tools"/><category term="natural-language-processing"/><category term="vietnamese"/><summary type="html"><![CDATA[We extract the most common words in Vietnamese by analyzing word frequencies of large documents]]></summary></entry><entry><title type="html">Hello World</title><link href="https://devintdha.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello World"/><published>2024-10-17T00:00:00+00:00</published><updated>2024-10-17T00:00:00+00:00</updated><id>https://devintdha.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://devintdha.github.io/blog/2024/hello-world/"><![CDATA[<p>Hello World! This is the first post on this blog, and I want to use it to capture my current state of mind for my future self. If you are reading this future me, then welcome back!</p> <p>Currently, I am working on my master‚Äôs thesis on Explainable AI. Due to the black box nature of deep learning models and the uncertainty of getting results, it‚Äôs making me a bit anxious. Pretty stressful with work on the side. But it‚Äôs going, and I have 24 weeks left at this point. Hopefully all goes well!</p> <p>Now for the reason I started this blog: I created some tools for Vietnamese language learning and wanted to publish my results. Could be of use to someone else! I already created a post, see its <a href="/projects/vietnamese-language-tools/">project page</a>.</p> <p>I am having a lot of fun learning Vietnamese. The feeling of learning new vocabulary and <em>finally</em> understanding some headlines after all these years feels great! Future-me, you‚Äôre still having a go at it, <a href="/assets/img/blog/other/duo_laser.png">I hope</a>?</p> <p>What‚Äôs also been lately on my mind is ‚Äúfuturistic‚Äù sounding music, and I can‚Äôt get enough of it. Ever since aespa released their new album‚Äôs title track <a href="https://www.youtube.com/watch?v=jWQx2f-CErU">Whiplash</a> I have been obsessed. The bass, the crunchy synths, and the drop at 2:46, where the minimalistic soundscape opens up, sound absolutely sick.</p> <p>This song made me create a new playlist called ‚ÄúClub Mix 30XX‚Äù (meaning in the year 3000-something). While looking for songs, I rediscovered the TRON: Legacy soundtrack. After all these years, it still sounds incredible to me and still very futuristic! I remember I listened to it when it came out, but there is actually a ‚Äúcomplete‚Äù edition with more tracks. I wasn‚Äôt aware of the track ‚ÄúCastor‚Äù before, but it fits perfectly into the playlist.</p> <p>Anyway, enough rambling. I need to go to sleep. Perhaps see you in a year or so?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The first post on this blog!]]></summary></entry></feed>